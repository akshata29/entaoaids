{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.schema.document import Document\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import display, HTML\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "#we'll use pickle to load text from our text cleaning notebook, but you can load any text into the full_text variable.  It should be stored as a single string.\n",
    "import pickle, os, re, json\n",
    "#we'll use time to track how long Bedrock takes to respond, which helps to estimate how long a job will take.\n",
    "import time\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from openai import OpenAI, AzureOpenAI, AsyncAzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\astalati\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.azure_openai.AzureChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "                azure_endpoint=os.getenv(\"OpenAiEndPoint\"),\n",
    "                api_version=os.getenv(\"OpenAiVersion\"),\n",
    "                azure_deployment=os.getenv(\"OpenAiChat4o\"),\n",
    "                temperature=0,\n",
    "                api_key=os.getenv(\"OpenAiKey\"),\n",
    "                openai_api_type=\"azure\",\n",
    "                max_tokens=2000)\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(azure_endpoint=os.getenv(\"OpenAiEndPoint\"), \n",
    "                                   azure_deployment=os.getenv('OpenAIEmbedding'), api_key=os.getenv(\"OpenAiKey\"), openai_api_type=\"azure\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"OpenAiKey\"),  \n",
    "    api_version=os.getenv(\"OpenAiVersion\"),\n",
    "    #base_url=f\"{os.getenv('OpenAiWestUsEp')}openai/deployments/{os.getenv('OpenAiGpt4v')}/extensions\",\n",
    "    azure_endpoint=os.getenv(\"OpenAiEndPoint\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrapping in a python function to make it easy to use in other scripts.\n",
    "def stuff_it_summary(llm, doc):\n",
    "    # Define prompt\n",
    "    prompt_template = \"\"\"\\n\\nHuman:  Consider this text:\n",
    "    <text>\n",
    "    {text}\n",
    "    </text>\n",
    "    Please create a concise summary in narative format.\n",
    "\n",
    "    Assistiant:  Here is the concise summary:\"\"\"\n",
    "    prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "    # Define LLM chain\n",
    "    llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    # Define StuffDocumentsChain\n",
    "    stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
    "\n",
    "    #Note that although langchain often stores douments in small chunks for the \n",
    "    #convience of models with smaller context windows, this \"stuff it\" method will\n",
    "    #combind all those chunks into a single prompt call.\n",
    "\n",
    "    if type(doc) == str:\n",
    "        docs = [Document(page_content=doc)]\n",
    "    return stuff_chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map\n",
    "map_template = \"\"\"\\n\\nHuman: The following is a set of documents\n",
    "<documnets>\n",
    "{docs}\n",
    "</documents>\n",
    "Based on this list of docs, please identify the main themes.\n",
    "\n",
    "Assistant:  Here are the main themes:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"\\n\\nHuman: The following is set of summaries:\n",
    "<summaries>\n",
    "{doc_summaries}\n",
    "</summaries>\n",
    "Please take these and distill them into a final, consolidated summary of the main themes in narative format. \n",
    "\n",
    "Assistant:  Here are the main themes:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "#wrapping in a python function to make it easy to use in other scripts.\n",
    "def map_reduce_summary(doc, DEBUG=False):\n",
    "    if type(doc) == str:\n",
    "        #use the LangChain built in text splitter to split our text\n",
    "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size = 5000,\n",
    "            chunk_overlap  = 200,\n",
    "            length_function = len,\n",
    "            add_start_index = True,\n",
    "        )\n",
    "        split_docs = text_splitter.create_documents([doc])\n",
    "        if DEBUG: print(\"Text was split into %s docs\"%len(split_docs))\n",
    "    return map_reduce_chain.run(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example count_tokens function (if you have your own, use it)\n",
    "def count_tokens(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(full_text, OVERLAP=True, DEBUG=False):\n",
    "    '''\n",
    "    This will take a text and return an array with sliced chunks of the text in optimal sizing for summarization.  Note that by default, this does include overlaping text in each chunk.\n",
    "    Overlaping allows more cohesion between text, and should only be turned off when trying to count specific numbers and no duplicated text is a requirment.\n",
    "    \n",
    "    We could just drop text up to the maximum context window of our model, but that actually doesn't work very well.\n",
    "    Part of the reason for this is because no matter the input length, the output length is about the same.\n",
    "    For example, if you drop in a paragraph or 10 pages, you get about a paragraph in response.\n",
    "    To mitigate this, we create chunks using the lesser of two values: 25% of the total token count or 2k tokens.\n",
    "    We'll also overlap our chunks by about a paragraph of text or so, in order to provide continuity between chunks.\n",
    "    (Logic taken from https://gist.github.com/Donavan/4fdb489a467efdc1faac0077a151407a)\n",
    "    '''\n",
    "    DEBUG = False #debugging at this level is usually not very helpful.\n",
    "    \n",
    "    #Following testing, it was found that chunks should be 2000 tokens, or 25% of the doc, whichever is shorter.\n",
    "    #max chunk size in tokens\n",
    "    chunk_length_tokens = 2000\n",
    "    #chunk length may be shortened later for shorter docs.\n",
    "    \n",
    "    #a paragraph is about 200 words, which is about 260 tokens on average\n",
    "    #we'll overlap our chunks by a paragraph to provide cohesion to the final summaries.\n",
    "    overlap_tokens = 260\n",
    "    if not OVERLAP: overlap_tokens = 0\n",
    "    \n",
    "    #anything this short doesn't need to be chunked further.\n",
    "    min_chunk_length = 260 + overlap_tokens*2\n",
    "    \n",
    "    \n",
    "    #grab basic info about the text to be chunked.\n",
    "    char_count = len(full_text)\n",
    "    word_count = len(full_text.split(\" \"))#rough estimate\n",
    "    token_count = count_tokens(full_text)\n",
    "    token_per_charater = token_count/char_count\n",
    "\n",
    "    \n",
    "    #don't chunk tiny texts\n",
    "    if token_count <= min_chunk_length:\n",
    "        if DEBUG: print(\"Text is too small to be chunked further\")\n",
    "        return [full_text]\n",
    "    \n",
    "    \n",
    "    if DEBUG:\n",
    "        print (\"Chunk DEBUG mode is on, information about the text and chunking will be printed out.\")\n",
    "        print (\"Estimated character count:\",char_count)\n",
    "        print (\"Estimated word count:\",word_count)\n",
    "        print (\"Estimated token count:\",token_count)\n",
    "        print (\"Estimated tokens per character:\",token_per_charater)\n",
    "\n",
    "        print(\"Full text tokens: \",count_tokens(full_text))\n",
    "        print(\"How many times bigger than max context window: \",round(count_tokens(full_text)/max_token_count,2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #if the text is shorter, use smaller chunks\n",
    "    if (token_count/4<chunk_length_tokens):\n",
    "        overlap_tokens = int((overlap_tokens/chunk_length_tokens)*int(token_count/4))\n",
    "        chunk_length_tokens = int(token_count/4)\n",
    "        \n",
    "        if DEBUG: \n",
    "            print(\"Short doc detected:\")\n",
    "            print(\"New chunk length:\",chunk_length_tokens)\n",
    "            print(\"New overlap length:\",overlap_tokens)\n",
    "        \n",
    "    #convert to charaters for easy slicing using our approximate tokens per character for this text.\n",
    "    overlap_chars = int(overlap_tokens/token_per_charater)\n",
    "    chunk_length_chars = int(chunk_length_tokens/token_per_charater)\n",
    "    \n",
    "    #itterate and create the chunks from the full text.\n",
    "    chunks = []\n",
    "    start_chunk = 0\n",
    "    end_chunk = chunk_length_chars + overlap_chars\n",
    "    \n",
    "    last_chunk = False\n",
    "    while not last_chunk:\n",
    "        #the last chunk may not be the full length.\n",
    "        if(end_chunk>=char_count):\n",
    "            end_chunk=char_count\n",
    "            last_chunk=True\n",
    "        chunks.append(full_text[start_chunk:end_chunk])\n",
    "        \n",
    "        #move our slice location\n",
    "        if start_chunk == 0:\n",
    "            start_chunk += chunk_length_chars - overlap_chars\n",
    "        else:\n",
    "            start_chunk += chunk_length_chars\n",
    "        \n",
    "        end_chunk = start_chunk + chunk_length_chars + 2 * overlap_chars\n",
    "        \n",
    "    if DEBUG:print (\"Created %s chunks.\"%len(chunks))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\\n\\nHuman:  I am going to give you a text{{GUIDANCE_1}}.  This text is extracted from a larger document.  Here is the text:\n",
    "\n",
    "<text>\n",
    "{{TEXT}}\n",
    "</text>\n",
    "{{GUIDANCE_2}}\n",
    "{{STYLE}}{{REQUEST}}{{FORMAT}}{{GUIDANCE_3}}\n",
    "\\nAssistant:  Here is what you asked for:\n",
    "\"\"\"\n",
    "\n",
    "merge_prompt_template = \"\"\"\\n\\nHuman:  Here are a number of related summaries:\n",
    "\n",
    "{{TEXT}}\n",
    "Please merge these summaries into a highly detailed single summary in {{FORMAT}} format, preserving as much detail as possible, using less than 1000 tokens.\n",
    "\\nAssistant:  Here is what you asked for:\n",
    "\"\"\"\n",
    "\n",
    "#this is inserted into the prompt template above, in the {{GUIDANCE_2}} section.\n",
    "guidance_tempate = \"\"\"\n",
    "Here is the additional guidance:\n",
    "<guidance>\n",
    "{{GUIDANCE}}\n",
    "</guidance>\n",
    "\"\"\"\n",
    "\n",
    "#this prompt asks the LLM to be a newpaper reporter, extracting facts from a document to be used in a later report.  Good for summarizing factual sets of documents.\n",
    "reporter_prompt = \"\"\"\\n\\nHuman:  You are a newspaper reporter, collecting facts to be used in writing an article later.  Consider this source text:\n",
    "<text>\n",
    "{{TEXT}}\n",
    "</text>\n",
    "{{DOCS_DESCRIPTION}}  Please create a {{FORMAT}} of all the relevant facts from this text which will be useful in answering the question \"{{GUIDANCE}}\".  To make your list as clear as possible, do not use and pronouns or ambigious phrases.  For example, use a company's name rather than saying \"the company\" or they.\n",
    "\\nAssistant:  Here is the {{FORMAT}} of relevant facts:\n",
    "\"\"\"\n",
    "\n",
    "reporter_summary_prompt = \"\"\"\\n\\nHuman:  You are a newspaper reporter, collecting facts to be used in writing an article later.  Consider these notes, each one derived from a different source text:\n",
    "{{TEXT}}\n",
    "Please create a {{FORMAT}} of all the relevant facts and trends from these notes which will be useful in answering the question \"{{GUIDANCE}}\"{{STYLE}}.  To make your list as clear as possible, do not use and pronouns or ambigious phrases.  For example, use a company's name rather than saying \"the company\" or \"they\".\n",
    "\\nAssistant:  Here is the list of relevant facts:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "reporter_final_prompt = \"\"\"\\n\\nHuman:  You are a newspaper reporter, writing an article based on facts that were collected and summarized earlier.  Consider these summaries:\n",
    "{{TEXT}}\n",
    "Each summary is a collection of facts extracted from a number of source reports.  Each source report was written by an AWS team talking about their interactions with their individual customer.  Please create a {{FORMAT}} of all the relevant trends and details from these summaries which will be useful in answering the question \"{{GUIDANCE}}\".\n",
    "\\nAssistant:  Here is the narrative:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt(text,prompt_type,format_type, manual_guidance, style_guide, docs_description=\"\"):\n",
    "    '''\n",
    "    text should be a single string of the raw text to be sent to the gen ai model.\n",
    "    prompt_type must be \"summary\" or \"interrogate\" or \"answers\"\n",
    "            -summary means summarize the text\n",
    "            -interrogate means look at the text and ask questions about what is missing\n",
    "            -answers means looking at the test, provide only details that may help answer the questions according to the Guidance.\n",
    "            -merge_answers takes a summary as text, and merges in the facts in the guidance section\n",
    "            -merge_summaries takes 2 or more summaries and merges them together.  The summaries to be merged must be in list format for best results.\n",
    "            -reporter - like a new reporter, extract details that help answer the guidance questions\n",
    "            -reporter_summary - like a news reporter looking at a bunch of notes, create a list summary.  Intended as an intermediate step. \n",
    "            reporter_final - generative a narrative based on the reporter_summary outputs.\n",
    "    format_type must be \"narrative\" or \"list\"\n",
    "    manual_guidance Extra instructions to guide the process, usually from the user.\n",
    "    style_guide TBD\n",
    "    \n",
    "    Note that merge_summaries is handled differntly than all other options because it iteratively adds in multiple texts.\n",
    "    '''\n",
    "    \n",
    "    #answers mode is a bit different, so handle that first.\n",
    "    if prompt_type == \"answers\":\n",
    "        format_type = \"in list format, using less than 1000 tokens.  \"\n",
    "        prompt_type = \"Please provide a list of any facts from the text that could be relevant to answering the questions from the guidance section \"\n",
    "        guidance_1 = \" and some guidance\"\n",
    "        guidance_2 = guidance_tempate.replace(\"{{GUIDANCE}}\",manual_guidance)\n",
    "        guidance_3 = \"You should ignore any questions that can not be answered by this text.\"\n",
    "    elif prompt_type == \"reporter\":\n",
    "        return reporter_prompt.replace(\"{{TEXT}}\",text).replace(\"{{FORMAT}}\",format_type).replace(\"{{GUIDANCE}}\",manual_guidance).replace(\"{{DOCS_DESCRIPTION}}\",docs_description)\n",
    "    elif prompt_type == \"reporter_summary\":\n",
    "        summaries_text = \"\"\n",
    "        for x,summary in enumerate(text):\n",
    "            summaries_text += \"<note_%s>\\n%s</note_%s>\\n\"%(x+1,summary,x+1)\n",
    "        final_prompt = reporter_summary_prompt.replace(\"{{TEXT}}\",summaries_text).replace(\"{{FORMAT}}\",format_type).replace(\"{{GUIDANCE}}\",manual_guidance).replace(\"{{STYLE}}\",style_guide)\n",
    "        return final_prompt\n",
    "    elif prompt_type == \"reporter_final\":\n",
    "        summaries_text = \"\"\n",
    "        for x,summary in enumerate(text):\n",
    "            summaries_text += \"<summary_%s>\\n%s</summary_%s>\\n\"%(x+1,summary,x+1)\n",
    "        final_prompt = reporter_final_prompt.replace(\"{{TEXT}}\",summaries_text).replace(\"{{FORMAT}}\",format_type).replace(\"{{GUIDANCE}}\",manual_guidance)\n",
    "        return final_prompt\n",
    "    elif prompt_type == \"merge_summaries\":\n",
    "        summaries_text = \"\"\n",
    "        for x,summary in enumerate(text):\n",
    "            summaries_text += \"<summary_%s>\\n%s</summary_%s>\\n\"%(x+1,summary,x+1)\n",
    "        final_prompt = merge_prompt_template.replace(\"{{TEXT}}\",summaries_text).replace(\"{{FORMAT}}\",format_type)\n",
    "        return final_prompt\n",
    "        \n",
    "    elif prompt_type == \"merge_answers\":\n",
    "        prompt_type = \"The text is a good summary which may lack a few details.  However, the additional information found in the guidance section can be used to make the summary even better.  Starting with the text, please use the details in the guidance section to make the text more detailed.  The new summary shoud use less than 1000 tokens.  \"\n",
    "        format_type = \"\"\n",
    "        guidance_1 = \" and some guidance\"\n",
    "        guidance_2 = guidance_tempate.replace(\"{{GUIDANCE}}\",manual_guidance)\n",
    "        guidance_3 = \"You should ignore any comments in the guidance section indicating that answers could not be found.\"\n",
    "    else:\n",
    "        #Based on the options passed in, grab the correct text to eventually use to build the prompt.\n",
    "        #select the correct type of output format desired, list or summary.  Note that list for interrogate prompts is empty because the request for list is built into that prompt.\n",
    "        if prompt_type == \"interrogate\" and format_type != \"list\":\n",
    "            raise ValueError(\"Only list format is supported for interrogate prompts.\")\n",
    "        if format_type == \"list\":\n",
    "            if prompt_type == \"interrogate\":\n",
    "                format_type = \"\"#already in the prompt so no format needed.\n",
    "            else:\n",
    "                format_type = \"in list format, using less than 1000 tokens.\"\n",
    "        elif format_type == \"narrative\":\n",
    "            format_type = \"in narrative format, using less than 1000 tokens.\"\n",
    "        else:\n",
    "            raise ValueError(\"format_type must be 'narrative' or 'list'.\")\n",
    "\n",
    "        #select the correct prompt type language\n",
    "        if prompt_type == \"summary\":\n",
    "            prompt_type = \"Please provide a highly detailed summary of this text \"\n",
    "        elif prompt_type == \"interrogate\":\n",
    "            prompt_type = \"This text is a summary that lacks detail.  Please provide a list of the top 10 most important questions about this text that can not be answered by the text.\"\n",
    "        else:\n",
    "            raise ValueError(\"prompt_type must be 'summary' or 'interrogate'.\")\n",
    "\n",
    "        if manual_guidance == \"\":\n",
    "            guidance_1 = \"\"\n",
    "            guidance_2 = \"\"\n",
    "            guidance_3 = \"\"\n",
    "        else:\n",
    "            guidance_1 = \" and some guidance\"\n",
    "            guidance_2 = guidance_tempate.replace(\"{{GUIDANCE}}\",manual_guidance)\n",
    "            guidance_3 = \"  As much as possible, also follow the guidance from the guidance section above.  You should ignore guidance that does not seem relevant to this text.\"\n",
    "        \n",
    "    #TBD\n",
    "    style_guide = \"\"\n",
    "    #print (prompt_template.replace(\"{{GUIDANCE_1}}\",guidance_1).replace(\"{{GUIDANCE_2}}\",guidance_2).replace(\"{{GUIDANCE_3}}\",guidance_3).replace(\"{{STYLE}}\",style_guide).replace(\"{{REQUEST}}\",prompt_type).replace(\"{{FORMAT}}\",format_type))\n",
    "    final_prompt = prompt_template.replace(\"{{TEXT}}\",text).replace(\"{{GUIDANCE_1}}\",guidance_1).replace(\"{{GUIDANCE_2}}\",guidance_2).replace(\"{{GUIDANCE_3}}\",guidance_3).replace(\"{{STYLE}}\",style_guide).replace(\"{{REQUEST}}\",prompt_type).replace(\"{{FORMAT}}\",format_type)\n",
    "    return final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_azure_openai(prompt_text, DEBUG=False):\n",
    "    '''\n",
    "    Send a prompt to Azure OpenAI, and return the response.\n",
    "    DEBUG is used to see exactly what is being sent to and from Azure OpenAI.\n",
    "    '''\n",
    "\n",
    "    # Ensure the prompt contains the expected format\n",
    "    if \"Assistant:\" not in prompt_text:\n",
    "        prompt_text = \"\\n\\nHuman:\" + prompt_text + \"\\nAssistant: \"\n",
    "\n",
    "    # Prompt payload for Azure OpenAI\n",
    "    prompt_json = {\n",
    "        \"prompt\": prompt_text,\n",
    "        \"max_tokens\": 3000,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.7,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"stop\": [\"\\n\\nHuman:\"]\n",
    "    }\n",
    "    \n",
    "    # Cache results if present\n",
    "    body = json.dumps(prompt_json)\n",
    "\n",
    "    if DEBUG:\n",
    "        print(\"sending:\", prompt_text)\n",
    "\n",
    "    start_time = time.time()\n",
    "    attempt = 1\n",
    "    MAX_ATTEMPTS = 3\n",
    "    while True:\n",
    "        try:\n",
    "            query_start_time = time.time()\n",
    "\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt_text}]\n",
    "\n",
    "            # Invoke the Azure OpenAI model\n",
    "            response = client.chat.completions.create(\n",
    "                messages=messages,\n",
    "                temperature=0,\n",
    "                max_tokens=500,\n",
    "                model=os.getenv(\"OpenAiChat4o\"),\n",
    "            )\n",
    "            \n",
    "            # Extract the result from the response\n",
    "            raw_results = response.choices[0].message.content.strip(\" \\n\")\n",
    "\n",
    "            # Remove any unwanted HTML-like tags if they appear\n",
    "            results = re.sub('<[^<]+?>', '', raw_results)\n",
    "\n",
    "            # Compute metrics\n",
    "            request_time = round(time.time() - start_time, 2)\n",
    "            if DEBUG:\n",
    "                print(\"Received:\", results)\n",
    "                print(\"request time (sec):\", request_time)\n",
    "\n",
    "            total_tokens = count_tokens(prompt_text + raw_results)  # Assuming you have a count_tokens function\n",
    "            output_tokens = count_tokens(raw_results)\n",
    "            tokens_per_sec = round(total_tokens / request_time, 2)\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with Azure OpenAI API call: {str(e)}\")\n",
    "            attempt += 1\n",
    "            if attempt > MAX_ATTEMPTS:\n",
    "                print(\"Max attempts reached!\")\n",
    "                results = str(e)\n",
    "                request_time = -1\n",
    "                total_tokens = -1\n",
    "                output_tokens = -1\n",
    "                tokens_per_sec = -1\n",
    "                break\n",
    "            else:\n",
    "                # Retry after 10 seconds\n",
    "                time.sleep(10)\n",
    "\n",
    "    return (prompt_text, results, total_tokens, output_tokens, request_time, tokens_per_sec, query_start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threaded function for queue processing.\n",
    "def thread_request(q, result):\n",
    "    while not q.empty():\n",
    "        work = q.get()                      #fetch new work from the Queue\n",
    "        thread_start_time = time.time()\n",
    "        try:\n",
    "            data = ask_azure_openai(work[1])\n",
    "            result[work[0]] = data          #Store data back at correct index\n",
    "        except Exception as e:\n",
    "            error_time = time.time()\n",
    "            print('Error with prompt!',str(e))\n",
    "            result[work[0]] = (work[1],str(e),count_tokens(work[1]),0,round(error_time-thread_start_time,2),0,thread_start_time)\n",
    "        #signal to the queue that task has been processed\n",
    "        q.task_done()\n",
    "    return True\n",
    "\n",
    "def ask_aoai_threaded(prompts,DEBUG=False):\n",
    "    '''\n",
    "    Call ask_claude, but multi-threaded.\n",
    "    Returns a dict of the prompts and responces.\n",
    "    '''\n",
    "    q = Queue(maxsize=0)\n",
    "    num_theads = min(50, len(prompts))\n",
    "    \n",
    "    #Populating Queue with tasks\n",
    "    results = [{} for x in prompts];\n",
    "    #load up the queue with the promts to fetch and the index for each job (as a tuple):\n",
    "    for i in range(len(prompts)):\n",
    "        #need the index and the url in each queue item.\n",
    "        q.put((i,prompts[i]))\n",
    "        \n",
    "    #Starting worker threads on queue processing\n",
    "    for i in range(num_theads):\n",
    "        #print('Starting thread ', i)\n",
    "        worker = Thread(target=thread_request, args=(q,results))\n",
    "        worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to \n",
    "                                  #exit eventually even if these dont finish \n",
    "                                  #correctly.\n",
    "        worker.start()\n",
    "\n",
    "    #now we wait until the queue has been processed\n",
    "    q.join()\n",
    "\n",
    "    if DEBUG:print('All tasks completed.')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_from_chunks(chunks, prompt_options,DEBUG=False, chunks_already_summarized=False):\n",
    "    \"\"\"\n",
    "    This function itterates through a list of chunks, summarizes them, then merges those summaries together into one.\n",
    "    chunks_already_summarized is used when the chunks passed in are chunks resulting from summerizing docs.\n",
    "    If the chunks are taken from a source document directly, chunks_already_summarized should be set to False.\n",
    "    \"\"\"\n",
    "    partial_summaries = {}\n",
    "    if not chunks_already_summarized:#chunks are from a source doc, so summarize them.\n",
    "        partial_summaries_prompts = []\n",
    "        partial_summaries_prompt2chunk = {}\n",
    "        for x,chunk in enumerate(chunks):\n",
    "            #if DEBUG: print (\"Working on chunk\",x+1,end = '')\n",
    "            start_chunk_time = time.time()\n",
    "            #note that partial summaries are always done in list format to maximize information captured.\n",
    "            custom_prompt = get_prompt(chunk,prompt_options['prompt_type'],'list', prompt_options['manual_guidance'], prompt_options['style_guide'])\n",
    "            #partial_summaries[chunk] = ask_claude(custom_prompt,DEBUG=False)\n",
    "            partial_summaries_prompts.append(custom_prompt)\n",
    "            partial_summaries_prompt2chunk[custom_prompt]=chunk\n",
    "        \n",
    "        partial_summaries_results = ask_aoai_threaded(partial_summaries_prompts)\n",
    "        for prompt_text,results,total_tokens,output_tokens,request_time,tokens_per_sec,query_start_time in partial_summaries_results:\n",
    "            partial_summaries[partial_summaries_prompt2chunk[prompt_text]] = results\n",
    "\n",
    "        if DEBUG: \n",
    "            print (\"Partial summary chunks done!\")\n",
    "            print (\"Creating joint summary...\")\n",
    "    else:\n",
    "        for chunk in chunks:\n",
    "            partial_summaries[chunk] = chunk\n",
    "        if DEBUG: \n",
    "            print (\"Summarized chunks detected!\")\n",
    "            print (\"Creating joint summary...\")\n",
    "            \n",
    "    summaries_list = []\n",
    "    summaries_list_token_count = 0\n",
    "    for chunk in chunks:\n",
    "        summaries_list.append(partial_summaries[chunk]) \n",
    "        summaries_list_token_count+=count_tokens(partial_summaries[chunk])\n",
    "        \n",
    "    if DEBUG: print(\"Chunk summaries token count:\",summaries_list_token_count)\n",
    "    \n",
    "    #check to see if the joint summary is too long.  If it is, recursivly itterate down.\n",
    "    #we do this, rather than chunking again, so that summaries are not split.\n",
    "    #it needs to be under 3000 tokens in order to be helpful to the summary (4000 is an expiremental number and may need to be adjusted.)\n",
    "    #this may be higher than the 2000 used for text originally, because this data is in list format.\n",
    "    recombine_token_target = 3000\n",
    "    #summaries_list_token_count = recombine_token_target+1 #set this to target+1 so that we do at least one recombonation for shorter documents.\n",
    "    while summaries_list_token_count>recombine_token_target:\n",
    "        if DEBUG: print(\"Starting reduction loop to merge chunks.  Total token count is %s\"%summaries_list_token_count)\n",
    "        new_summaries_list = []\n",
    "        summaries_list_token_count = 0\n",
    "        temp_summary_group = []\n",
    "        temp_summary_group_token_length = 0\n",
    "        for summary in summaries_list:\n",
    "            if temp_summary_group_token_length + count_tokens(summary) > recombine_token_target:\n",
    "                #the next summary added would push us over the edge, so summarize the current list, and then add it.\n",
    "                #note that partial summaries are always done in list format to maximize information captured.\n",
    "                if DEBUG: print(\"Reducing %s partial summaries into one...\"%(len(temp_summary_group)))\n",
    "                custom_prompt = get_prompt(temp_summary_group,\"merge_summaries\",\"list\", prompt_options['manual_guidance'], prompt_options['style_guide'])\n",
    "                temp_summary = ask_azure_openai(custom_prompt,DEBUG=False)[1]\n",
    "                new_summaries_list.append(temp_summary)\n",
    "                summaries_list_token_count+= count_tokens(temp_summary)\n",
    "                temp_summary_group = []\n",
    "                temp_summary_group_token_length = 0\n",
    "            \n",
    "            temp_summary_group.append(summary)\n",
    "            temp_summary_group_token_length+= count_tokens(summary)\n",
    "        \n",
    "        #summarize whever extra summaries are still in the temp list\n",
    "        if len(temp_summary_group)>1:\n",
    "            if DEBUG: print(\"Starting final reduction of %s partial summaries into one...\"%(len(temp_summary_group)))\n",
    "            custom_prompt = get_prompt(temp_summary_group,\"merge_summaries\",\"list\", prompt_options['manual_guidance'], prompt_options['style_guide'])\n",
    "            temp_summary = ask_azure_openai(custom_prompt,DEBUG=False)[1]\n",
    "            new_summaries_list.append(temp_summary)\n",
    "            summaries_list_token_count+= count_tokens(temp_summary)\n",
    "        elif len(temp_summary_group)==1:\n",
    "            if DEBUG: print(\"Tacking on an extra partial summary\")\n",
    "            new_summaries_list.append(temp_summary_group[0])\n",
    "            summaries_list_token_count+= count_tokens(temp_summary_group[0])\n",
    "            \n",
    "        summaries_list = new_summaries_list\n",
    "        \n",
    "    if DEBUG: print (\"Final merge of summary chunks, merging %s summaries.\"%(len(summaries_list)))\n",
    "    custom_prompt = get_prompt(summaries_list,\"merge_summaries\",prompt_options['format_type'], prompt_options['manual_guidance'], prompt_options['style_guide'])\n",
    "    full_summary = ask_azure_openai(custom_prompt,DEBUG=False)[1]\n",
    "    #full_summary_prompt = get_prompt(\"/n\".join(summaries_list),prompt_options['prompt_type'],prompt_options['format_type'], prompt_options['manual_guidance'], prompt_options['style_guide'])\n",
    "    #full_summary = ask_claude(full_summary_prompt,DEBUG=False)\n",
    "    \n",
    "    return full_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single_doc_summary(full_text, prompt_options,AUTO_REFINE=True, DEBUG=False,ALREADY_CHUNKED_AND_SUMMED=False):\n",
    "    \"\"\"\n",
    "    This function uses the three helper functions, as well as the generate_summary_from_chunks above, to iteratively generate high quality summaries.\n",
    "    AUTO_REFINE, if true, has the LLM generate a list of questions, and then recursivly calls this function with those questions for guidance.\n",
    "    ALREADY_CHUNKED_AND_SUMMED, if true, means that this is being called using a list of summarized documents which should not be chunked or summarized further.\n",
    "    \"\"\"\n",
    "    #first break this document into chunks\n",
    "    chunks = []        \n",
    "    \n",
    "    if ALREADY_CHUNKED_AND_SUMMED:\n",
    "        chunks = full_text\n",
    "    else:\n",
    "        chunks = get_chunks(full_text,DEBUG=DEBUG)\n",
    "        \n",
    "    if DEBUG:\n",
    "        if prompt_options['prompt_type'] == \"answers\":\n",
    "            print (\"Generating answers using %s chunks.\"%(len(chunks)))\n",
    "        else:\n",
    "            print (\"Generating a new combined summary for %s chunks.\"%(len(chunks)))\n",
    "        if ALREADY_CHUNKED_AND_SUMMED:\n",
    "            print (\"Input has already been chunked and summarized, skipping initial chunking.\")\n",
    "        \n",
    "            \n",
    "    first_summary = generate_summary_from_chunks(chunks,prompt_options,DEBUG=DEBUG, chunks_already_summarized=ALREADY_CHUNKED_AND_SUMMED)\n",
    "    \n",
    "    if DEBUG and AUTO_REFINE: \n",
    "        print (\"First summary:\")\n",
    "        print (first_summary)\n",
    "        \n",
    "    if AUTO_REFINE: \n",
    "        if DEBUG: print (\"Asking the LLM to find weaknesses in this summary...\")\n",
    "        #now that we have a rough summary, let's grab some questions about it.\n",
    "        questions_prompt = get_prompt(first_summary,\"interrogate\",\"list\", \"\", \"\")\n",
    "        questions_list = ask_azure_openai(questions_prompt,DEBUG=False)[1]\n",
    "\n",
    "        if DEBUG: \n",
    "            print (\"Questions from the LLM:\")\n",
    "            print (questions_list)\n",
    "            \n",
    "        original_guidance = prompt_options['manual_guidance']\n",
    "        original_prompt_type = prompt_options['prompt_type']\n",
    "        prompt_options['manual_guidance'] = prompt_options['manual_guidance'] + questions_list\n",
    "        prompt_options['prompt_type'] = \"answers\"\n",
    "        add_details = generate_single_doc_summary(full_text, prompt_options,AUTO_REFINE=False, DEBUG=DEBUG, ALREADY_CHUNKED_AND_SUMMED=ALREADY_CHUNKED_AND_SUMMED)\n",
    "        if DEBUG: \n",
    "            print(\"Additional Details:\")\n",
    "            print (add_details)\n",
    "            print(\"Merging details into original summary...\")\n",
    "        \n",
    "        prompt_options['manual_guidance'] = original_guidance + add_details\n",
    "        prompt_options['prompt_type'] = \"merge_answers\"\n",
    "        custom_prompt = get_prompt(first_summary,prompt_options['prompt_type'],prompt_options['format_type'], prompt_options['manual_guidance'], prompt_options['style_guide'])\n",
    "        final_summary = ask_azure_openai(custom_prompt,DEBUG=False)[1]\n",
    "        \n",
    "        #return this back to the original to prevent weird errors between calls of this function.\n",
    "        prompt_options['manual_guidance'] = original_guidance\n",
    "        prompt_options['prompt_type'] = original_prompt_type\n",
    "        return final_summary\n",
    "    \n",
    "    else:\n",
    "        return first_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_set_chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\n",
    "    This is a helper function for the multidoc summarization function.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_docs_summary(docs, questions, docs_description, DEBUG=False):\n",
    "    \"\"\"\n",
    "    This function uses the three helper functions to read the documents passed in, and create a summary answer for each question passed in.\n",
    "    If the documents are longer than two pages or so, it is reccoemended that you first summaize each document.\n",
    "    docs_description is a single sentance describing what the documents are such as \"The texts are a collection of product reviews for a Pickle Ball paddle.\"\n",
    "    \"\"\"\n",
    "    #get answers from each doc for each question.\n",
    "    answers = {}\n",
    "    prompt2quetion_doc = {}\n",
    "    prompts = []\n",
    "    max_docs_to_scan = 500\n",
    "    \n",
    "    #build the queries to be passed into Bedrock\n",
    "    for question in questions:\n",
    "        for x,doc in enumerate(docs):\n",
    "            if x>max_docs_to_scan:break#limit for testing\n",
    "            \n",
    "            #print (\"Asking the LLM to find extract answers from this doc:\",doc)\n",
    "            questions_prompt = get_prompt(docs[doc],\"reporter\",\"list\", question, \"\",docs_description)\n",
    "            prompt2quetion_doc[questions_prompt] = (question,doc) \n",
    "            prompts.append(questions_prompt)\n",
    "        \n",
    "    if DEBUG:print(\"Starting %s worker threads.\"%len(prompts))\n",
    "    prompts_answers = ask_aoai_threaded(prompts,DEBUG=False)\n",
    "    \n",
    "    for question in questions:\n",
    "        answers[question] = []    \n",
    "    \n",
    "    for prompt,answer,total_tokens,output_tokens,request_time,tokens_per_sec,query_start_time in prompts_answers:\n",
    "        question,doc = prompt2quetion_doc[prompt]\n",
    "        answers[question].append(answer)\n",
    "        \n",
    "    \n",
    "    current_answer_count = len(docs)\n",
    "    if DEBUG: print(\"All documents have been read.  Reducing answers into the final summary...\")\n",
    "    #reduce this down to 5 or less docs for the final summary by combining the individual answers.\n",
    "    while current_answer_count > 5:\n",
    "        #summarize the answers\n",
    "        prompts = []\n",
    "        prompts2question = {}\n",
    "        \n",
    "        max_docs_to_scan = max(min(current_answer_count,8),3)\n",
    "        if DEBUG: print(\"Combining %s chunks.  (Currently there are %s answers to each question.)\"%(max_docs_to_scan,current_answer_count))\n",
    "        for question in questions:\n",
    "            #print (\"Asking the LLM to summarize answers for this question:\",question)\n",
    "            #You want chunks of roughly 2K tokens\n",
    "            for partial_chunks in grab_set_chunks(answers[question],max_docs_to_scan):\n",
    "                questions_prompt = get_prompt(partial_chunks,\"reporter_summary\",\"list\", question, \" in less than 1000 tokens\")\n",
    "                prompts.append(questions_prompt)\n",
    "                prompts2question[questions_prompt] = question\n",
    "        \n",
    "        if DEBUG:print(\"Starting %s worker threads.\"%len(prompts))\n",
    "        prompts_answers = ask_aoai_threaded(prompts,DEBUG=False)\n",
    "        \n",
    "        for question in questions:\n",
    "            answers[question] = []    \n",
    "        for prompt,answer,total_tokens,output_tokens,request_time,tokens_per_sec,query_start_time in prompts_answers:\n",
    "            answers[prompts2question[prompt]].append(answer)        \n",
    "\n",
    "        current_answer_count = len(answers[questions[0]])\n",
    "        \n",
    "    if DEBUG: print(\"Creating the final summary for each question.\")\n",
    "    #write the final article:\n",
    "    prompts = []\n",
    "    prompts2question = {}\n",
    "    for question in questions:\n",
    "        #print (\"Asking the LLM to finalize the answer for this question:\",question)\n",
    "        questions_prompt = get_prompt(answers[question],\"reporter_final\",\"narrative\", question, \"\")\n",
    "        prompts.append(questions_prompt)\n",
    "        prompts2question[questions_prompt] = question\n",
    "\n",
    "    if DEBUG:print(\"Starting %s worker threads.\"%len(prompts))\n",
    "    prompts_answers = ask_aoai_threaded(prompts,DEBUG=False)\n",
    "    \n",
    "    answers = {}\n",
    "    for prompt,answer,total_tokens,output_tokens,request_time,tokens_per_sec,query_start_time in prompts_answers:\n",
    "        answers[prompts2question[prompt]] = answer\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_open_short = './Data/Pickle/hills.pkl'  #2-3 page story, Hills like White Elephants\n",
    "text_to_open_mid = './Data/Pickle/algernon.pkl'  #short story, Flowers for Algernon\n",
    "text_to_open_long = './Data/Pickle/frankenstien.pkl' #short novel, Frankenstine\n",
    "text_to_open_short_factual = './Data/Pickle/elvis.pkl'  #longest wikipedia article, Elvis.\n",
    "\n",
    "with open(text_to_open_short, 'rb') as file:\n",
    "    #note that here, we're loading a single text, but the examples below require each text to be in a list.\n",
    "    doc = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a small, sun-drenched train station between Barcelona and Madrid, an American man and a girl named Jig sit at a table in the shade, waiting for their train. They order drinks and engage in a seemingly casual conversation that gradually reveals underlying tensions. Jig comments on the distant hills, likening them to white elephants, which leads to a discussion about trying new drinks and the simplicity of life. However, their dialogue soon shifts to a more serious and ambiguous topic, hinting at an \"operation\" the man wants Jig to undergo, which is implied to be an abortion. The man tries to reassure her that everything will be fine afterward, but Jig expresses doubts and a sense of loss. Despite his insistence that he only wants her to be happy, Jig feels conflicted and ultimately asks him to stop talking. As the train's arrival approaches, the man moves their bags, and upon returning, finds Jig smiling and claiming to feel fine, though the tension between them remains unresolved.\n"
     ]
    }
   ],
   "source": [
    "text_to_open_short = './Data/Pickle/hills.pkl'  #2-3 page story, Hills like White Elephants\n",
    "with open(text_to_open_short, 'rb') as file:\n",
    "    #note that here, we're loading a single text, but the examples below require each text to be in a list.\n",
    "    doc = pickle.load(file)\n",
    "\n",
    "stuffSummary = stuff_it_summary(llm, doc)\n",
    "print(stuffSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text was split into 2 docs\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmap_reduce_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEBUG\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[5], line 63\u001b[0m, in \u001b[0;36mmap_reduce_summary\u001b[1;34m(doc, DEBUG)\u001b[0m\n\u001b[0;32m     61\u001b[0m     split_docs \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39mcreate_documents([doc])\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText was split into \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m docs\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mlen\u001b[39m(split_docs))\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_reduce_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_docs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\base.py:545\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    546\u001b[0m         _output_key\n\u001b[0;32m    547\u001b[0m     ]\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    551\u001b[0m         _output_key\n\u001b[0;32m    552\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\base.py:378\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    371\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    376\u001b[0m }\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    160\u001b[0m     )\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\combine_documents\\base.py:137\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m    136\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[1;32m--> 137\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\combine_documents\\map_reduce.py:226\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[1;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine_docs\u001b[39m(\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    216\u001b[0m     docs: List[Document],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    220\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Combine documents in a map reduce manner.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    Combine by mapping first chain over all documents, then reducing the results.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m    This reducing can be done recursively if needed (if there are many documents).\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m     map_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# FYI - this is parallelized and so it is fast.\u001b[39;49;00m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     question_result_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39moutput_key\n\u001b[0;32m    232\u001b[0m     result_docs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    233\u001b[0m         Document(page_content\u001b[38;5;241m=\u001b[39mr[question_result_key], metadata\u001b[38;5;241m=\u001b[39mdocs[i]\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;66;03m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(map_results)\n\u001b[0;32m    236\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\llm.py:227\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[1;34m(self, input_list, callbacks)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    226\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 227\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    228\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)\n\u001b[0;32m    229\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs})\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\llm.py:224\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[1;34m(self, input_list, callbacks)\u001b[0m\n\u001b[0;32m    219\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    220\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    221\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_list},\n\u001b[0;32m    222\u001b[0m )\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 224\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    226\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chains\\llm.py:115\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    113\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    123\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    124\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\language_models\\chat_models.py:546\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    540\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    544\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    545\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\language_models\\chat_models.py:412\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    408\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    409\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    411\u001b[0m ]\n\u001b[1;32m--> 412\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_combine_llm_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m generations \u001b[38;5;241m=\u001b[39m [res\u001b[38;5;241m.\u001b[39mgenerations \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[0;32m    414\u001b[0m output \u001b[38;5;241m=\u001b[39m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations, llm_output\u001b[38;5;241m=\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_community\\chat_models\\openai.py:377\u001b[0m, in \u001b[0;36mChatOpenAI._combine_llm_outputs\u001b[1;34m(self, llm_outputs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m token_usage\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m overall_token_usage:\n\u001b[1;32m--> 377\u001b[0m         \u001b[43moverall_token_usage\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    379\u001b[0m         overall_token_usage[k] \u001b[38;5;241m=\u001b[39m v\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "print(map_reduce_summary(doc, DEBUG=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a new combined summary for 4 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\astalati\\AppData\\Local\\Temp\\ipykernel_23860\\3580541671.py:36: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
      "  worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial summary chunks done!\n",
      "Creating joint summary...\n",
      "Chunk summaries token count: 1523\n",
      "Final merge of summary chunks, merging 4 summaries.\n",
      "First summary:\n",
      "In the valley of the Ebro, characterized by long, white hills devoid of shade and trees, a train station sits exposed to the sun. The station building offers a warm shadow, and a bamboo bead curtain hangs across the open door of the bar to keep out flies. At a table in the shade outside the station building, an American man and a girl named Jig are waiting for the express train from Barcelona, which will arrive in forty minutes and stop for two minutes before continuing to Madrid. The scene is hot, and the atmosphere is tense.\n",
      "\n",
      "The girl asks what they should drink, and the man suggests beer. They order two large beers, which a woman from the bar brings to their table along with felt pads. The girl observes the white hills and comments that they look like white elephants, leading to a brief exchange where the man dismisses her observation. The girl then notices something painted on the bead curtain and asks what it says. The man explains it says \"Anis del Toro,\" a type of drink, and they decide to try it, ordering two with water. The girl comments that the drink tastes like licorice and makes a broader statement that everything tastes like licorice, especially things one has waited a long time for, like absinthe. The man reacts dismissively, telling her to \"cut it out.\"\n",
      "\n",
      "As the warm wind blows the bead curtain against the table, creating a relaxed yet somewhat tense atmosphere, the man brings up a \"simple operation,\" implied to be an abortion. He tries to reassure the girl, saying it’s not really an operation and is just to \"let the air in.\" The girl looks at the ground and does not respond immediately. The man continues to reassure her, saying he will stay with her and that it’s a natural process. The girl asks what they will do afterward, and the man assures her that they will be fine and happy, just like before. The girl expresses doubt, questioning what makes him think they will be happy again.\n",
      "\n",
      "The girl stands up and observes the scenery: fields of grain, trees along the Ebro river, and distant mountains. She reflects on the potential they have to enjoy everything around them. They discuss the idea of having everything and the world, with the girl expressing doubt and saying it's no longer theirs. The man tries to reassure her that it hasn't been taken away. The girl expresses a sense of resignation, saying she doesn't feel any particular way but just knows things. The man reiterates\n",
      "Asking the LLM to find weaknesses in this summary...\n",
      "Questions from the LLM:\n",
      "Certainly! Here are the top 10 most important questions about the text that cannot be answered by the text itself:\n",
      "\n",
      "1. What is the larger context or background of the relationship between the American man and the girl named Jig?\n",
      "2. Why are the American man and Jig traveling to Madrid?\n",
      "3. What is the significance of the white hills looking like white elephants to Jig?\n",
      "4. What is the history or backstory behind the \"simple operation\" (implied to be an abortion) that the man is referring to?\n",
      "5. How does Jig truly feel about the \"simple operation\" and the man's insistence on it?\n",
      "6. What are the long-term implications for the relationship between the American man and Jig after the operation?\n",
      "7. Why does the man dismiss Jig's observations and comments so frequently?\n",
      "8. What is the significance of the drink \"Anis del Toro\" in the context of their conversation?\n",
      "9. What are the deeper emotional and psychological states of both characters during this conversation?\n",
      "10. How does the setting (the valley of the Ebro, the train station, the bead curtain) contribute to the overall theme and mood of the story?\n",
      "\n",
      "These questions delve into the underlying themes, character motivations, and broader context that are not explicitly addressed in the text.\n",
      "Generating answers using 4 chunks.\n",
      "Partial summary chunks done!\n",
      "Creating joint summary...\n",
      "Chunk summaries token count: 1419\n",
      "Final merge of summary chunks, merging 4 summaries.\n",
      "Additional Details:\n",
      "In a sun-drenched train station nestled between two lines of rails, the American man and a girl named Jig sit at a table in the shade outside the building. The hills across the valley of the Ebro are long and white, devoid of shade and trees on this side. A curtain made of strings of bamboo beads hangs across the open door into the bar, keeping out flies. The express from Barcelona is due to arrive in forty minutes, stopping briefly before continuing to Madrid.\n",
      "\n",
      "The couple's conversation reveals a close, possibly romantic relationship, with the man expressing his love for Jig: “I love you now. You know I love you.” Jig, seeking reassurance, asks, “And if I do it you’ll be happy and things will be like they were and you’ll love me?” This dialogue hints at underlying tension and an important decision they face.\n",
      "\n",
      "Jig suggests they drink beer, and the man orders two large beers. When the woman brings the drinks, Jig comments that the hills look like white elephants, sparking a brief exchange about the metaphor. She later clarifies, “They don’t really look like white elephants. I just meant the coloring of their skin through the trees.” This imagery sets the tone for their conversation, filled with symbolic undertones.\n",
      "\n",
      "The man refers to a \"simple operation,\" implied to be an abortion, and tries to reassure Jig: “It’s really an awfully simple operation, Jig. It’s not really an operation at all. It’s just to let the air in.” Jig’s feelings about the operation are not explicitly stated, but her hesitation and contemplative silence speak volumes. She questions the man’s assurances: “And you think then we’ll be all right and be happy.”\n",
      "\n",
      "The man believes the operation will resolve their issues: “That’s the only thing that bothers us. It’s the only thing that’s made us unhappy.” However, Jig is skeptical about the future: “What makes you think so?” This exchange highlights the conflict between them, with the man focused on convincing Jig of the operation's simplicity and necessity, while Jig appears conflicted and distressed.\n",
      "\n",
      "Their conversation touches on trying new drinks, with Jig commenting on the taste of Anis del Toro: “It tastes like licorice.” She makes a broader statement about things tasting like licorice, especially things waited for, like absinthe. The man dismisses her observations: “Oh, cut it out,” indicating a possible lack of empathy or understanding of\n",
      "Merging details into original summary...\n",
      "Final Summary:\n",
      "In the valley of the Ebro, characterized by long, white hills devoid of shade and trees, a sun-drenched train station sits exposed to the sun. The station building offers a warm shadow, and a bamboo bead curtain hangs across the open door of the bar to keep out flies. At a table in the shade outside the station building, an American man and a girl named Jig are waiting for the express train from Barcelona, which will arrive in forty minutes and stop for two minutes before continuing to Madrid. The scene is hot, and the atmosphere is tense.\n",
      "\n",
      "The couple's conversation reveals a close, possibly romantic relationship, with the man expressing his love for Jig: “I love you now. You know I love you.” Jig, seeking reassurance, asks, “And if I do it you’ll be happy and things will be like they were and you’ll love me?” This dialogue hints at underlying tension and an important decision they face.\n",
      "\n",
      "The girl asks what they should drink, and the man suggests beer. They order two large beers, which a woman from the bar brings to their table along with felt pads. The girl observes the white hills and comments that they look like white elephants, leading to a brief exchange where the man dismisses her observation. She later clarifies, “They don’t really look like white elephants. I just meant the coloring of their skin through the trees.” This imagery sets the tone for their conversation, filled with symbolic undertones. The girl then notices something painted on the bead curtain and asks what it says. The man explains it says \"Anis del Toro,\" a type of drink, and they decide to try it, ordering two with water. The girl comments that the drink tastes like licorice and makes a broader statement that everything tastes like licorice, especially things one has waited a long time for, like absinthe. The man reacts dismissively, telling her to \"cut it out.\"\n",
      "\n",
      "As the warm wind blows the bead curtain against the table, creating a relaxed yet somewhat tense atmosphere, the man brings up a \"simple operation,\" implied to be an abortion. He tries to reassure the girl, saying it’s not really an operation and is just to \"let the air in.\" The girl looks at the ground and does not respond immediately. The man continues to reassure her, saying he will stay with her and that it’s a natural process. The girl asks what they will do afterward, and the man assures her that they will\n"
     ]
    }
   ],
   "source": [
    "prompt_options = {}\n",
    "prompt_options['prompt_type'] = \"summary\"\n",
    "prompt_options['format_type'] = \"narrative\"\n",
    "prompt_options['manual_guidance'] = \"\"\n",
    "prompt_options['style_guide'] = \"\"\n",
    "\n",
    "revised_summary = generate_single_doc_summary(doc, prompt_options, AUTO_REFINE=True, DEBUG=True)\n",
    "print (\"Final Summary:\")\n",
    "print (revised_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_to_open_long, 'rb') as file:\n",
    "    #note that here, we're loading a single text, but the examples below require each text to be in a list.\n",
    "    longdoc = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a new combined summary for 38 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\astalati\\AppData\\Local\\Temp\\ipykernel_23860\\3580541671.py:36: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
      "  worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial summary chunks done!\n",
      "Creating joint summary...\n",
      "Chunk summaries token count: 13145\n",
      "Starting reduction loop to merge chunks.  Total token count is 13145\n",
      "Reducing 8 partial summaries into one...\n",
      "Reducing 9 partial summaries into one...\n",
      "Reducing 8 partial summaries into one...\n",
      "Reducing 8 partial summaries into one...\n",
      "Starting final reduction of 5 partial summaries into one...\n",
      "Final merge of summary chunks, merging 5 summaries.\n",
      "First summary:\n",
      "In the late 17th century, R. Walton embarks on an ambitious expedition towards the North Pole, driven by childhood dreams of exploration and scientific discovery. Writing to his sister, Margaret Saville, he describes his arrival in St. Petersburgh, invigorated by the cold northern breeze and filled with optimism about uncovering the secrets of the magnet and making celestial observations. Despite enduring six years of preparation and hardships, Walton remains resolute, though he laments the absence of a friend to share his journey.\n",
      "\n",
      "Meanwhile, Victor Frankenstein, the narrator, leaves a court that had provided refuge for the night, walking the streets in fear of encountering a dreaded wretch. Drenched by rain and burdened by a heavy mental load, he finds solace upon meeting his dear friend, Henry Clerval, at an inn. Clerval, concerned about Victor's ill appearance, learns that Victor has been deeply engaged in an occupation that deprived him of rest. Victor's fears of the creature he left in his apartment are alleviated when he finds the room empty, but he soon suffers a nervous breakdown, leading to a prolonged illness. Clerval nurses him back to health, and Victor expresses deep gratitude.\n",
      "\n",
      "Victor receives a letter from his cousin, Elizabeth, who urges him to write and describes the family's situation, including the well-being of his father and brother Ernest. She recounts the story of Justine Moritz, a servant in their household who faced trials and tribulations. Justine, mistreated by her mother, was taken in by Victor's aunt and received a good education. Despite her hardships, Justine remains a beloved member of the family.\n",
      "\n",
      "In a remote cottage, an old man named De Lacey, his son Felix, and daughter Agatha live a humble life. The blind De Lacey spends his days playing an instrument and walking with Felix, while Agatha manages household tasks. The family, though gentle and loving, suffers from poverty and hunger, often sacrificing their own food for De Lacey. An observer, who longs to join them but fears rejection, secretly aids the family by gathering food and wood, earning their gratitude as a \"good spirit.\" The observer learns their language and feels a deep emotional connection to the family, sharing in their joys and sorrows.\n",
      "\n",
      "Victor, hoping to protect his family from danger, leaves his native country in late September, accompanied by Clerval. They travel through picturesque landscapes, with Clerval enchanted by the natural beauty, while Victor remains\n",
      "Asking the LLM to find weaknesses in this summary...\n",
      "Questions from the LLM:\n",
      "Certainly! Here are the top 10 most important questions about the text that cannot be answered by the text itself:\n",
      "\n",
      "1. What specific scientific discoveries or observations is R. Walton hoping to make on his expedition to the North Pole?\n",
      "2. What are the exact circumstances that led Victor Frankenstein to fear encountering a \"dreaded wretch\"?\n",
      "3. What occupation was Victor Frankenstein deeply engaged in that deprived him of rest?\n",
      "4. What is the nature and origin of the creature that Victor fears and left in his apartment?\n",
      "5. What specific events led to Victor's nervous breakdown and prolonged illness?\n",
      "6. What are the detailed contents of the letter from Elizabeth to Victor, beyond the brief summary provided?\n",
      "7. What trials and tribulations did Justine Moritz face, and how did they impact her life and relationship with Victor's family?\n",
      "8. Who is the observer that aids the De Lacey family, and what motivates them to help in secret?\n",
      "9. What specific dangers is Victor hoping to protect his family from by leaving his native country?\n",
      "10. How does the journey through picturesque landscapes affect the relationship between Victor and Clerval, and what are their interactions during this travel?\n",
      "\n",
      "These questions delve deeper into the characters' motivations, backgrounds, and the specific events that shape the narrative, which are not fully detailed in the provided text.\n",
      "Generating answers using 38 chunks.\n",
      "Partial summary chunks done!\n",
      "Creating joint summary...\n",
      "Chunk summaries token count: 13839\n",
      "Starting reduction loop to merge chunks.  Total token count is 13839\n",
      "Reducing 8 partial summaries into one...\n",
      "Reducing 8 partial summaries into one...\n",
      "Reducing 8 partial summaries into one...\n",
      "Reducing 8 partial summaries into one...\n",
      "Starting final reduction of 6 partial summaries into one...\n",
      "Final merge of summary chunks, merging 5 summaries.\n",
      "Additional Details:\n",
      "Victor Frankenstein's life is a tumultuous journey marked by ambition, fear, and profound loss. His story begins with R. Walton's expedition to the North Pole, driven by a passionate enthusiasm for discovery and a deep desire for companionship. Walton, who has prepared for this voyage for six years, expresses a longing for a friend to share his experiences, highlighting the severe absence of such a bond in his life.\n",
      "\n",
      "Victor's background is rooted in a close-knit family. His father, a friend of Beaufort, marries Caroline Beaufort after her father's death, and they have Victor, their eldest child, in Naples. Victor's childhood is filled with love and indulgence, shared with his close companion, Elizabeth Lavenza, who was adopted into the family, and his friend Henry Clerval, passionate about chivalry and adventure.\n",
      "\n",
      "Victor's early life is marked by a deep interest in learning the secrets of heaven and earth, influenced by the works of Cornelius Agrippa, Paracelsus, and Albertus Magnus. A thunderstorm and the destruction of an oak tree by lightning shift his interest towards electricity and galvanism. He attends the University of Ingolstadt, where his departure is delayed by family concerns.\n",
      "\n",
      "Victor's life takes a dark turn when he creates a creature in a fit of \"enthusiastic madness.\" The creature, feeling abandoned and cursed by his creator, vows revenge against Victor. Victor is haunted by the creature's presence, especially after the murder of his brother William, which he attributes to the creature. This fear leads to a nervous breakdown and prolonged illness, during which Clerval nurses him back to health.\n",
      "\n",
      "Victor's promise to create a female companion for the creature weighs heavily on him. He travels to England with Clerval, hoping to gain crucial knowledge, but is deeply repulsed by the task. His emotional state deteriorates, and he ultimately destroys the second creature, leading to a confrontation with the creature, who threatens to be with him on his wedding night.\n",
      "\n",
      "Victor's fears are realized when the creature kills Elizabeth on their wedding night. This loss, along with the deaths of William and Justine Moritz, who was executed for a crime she didn't commit, plunges Victor into profound shock and illness. He is later found in a state of utter exhaustion and kept in a solitary cell for many months, reflecting on the destruction caused by the creature and his immense guilt and remorse.\n",
      "\n",
      "Throughout his journey, Victor is consumed by a desire to protect his\n",
      "Merging details into original summary...\n",
      "Final Summary:\n",
      "In the late 17th century, R. Walton embarks on an ambitious expedition towards the North Pole, driven by childhood dreams of exploration and scientific discovery. Writing to his sister, Margaret Saville, he describes his arrival in St. Petersburgh, invigorated by the cold northern breeze and filled with optimism about uncovering the secrets of the magnet and making celestial observations. Despite enduring six years of preparation and hardships, Walton remains resolute, though he laments the absence of a friend to share his journey, highlighting the severe absence of such a bond in his life.\n",
      "\n",
      "Meanwhile, Victor Frankenstein, the narrator, leaves a court that had provided refuge for the night, walking the streets in fear of encountering a dreaded wretch. Drenched by rain and burdened by a heavy mental load, he finds solace upon meeting his dear friend, Henry Clerval, at an inn. Clerval, concerned about Victor's ill appearance, learns that Victor has been deeply engaged in an occupation that deprived him of rest. Victor's fears of the creature he left in his apartment are alleviated when he finds the room empty, but he soon suffers a nervous breakdown, leading to a prolonged illness. Clerval nurses him back to health, and Victor expresses deep gratitude.\n",
      "\n",
      "Victor receives a letter from his cousin, Elizabeth, who urges him to write and describes the family's situation, including the well-being of his father and brother Ernest. She recounts the story of Justine Moritz, a servant in their household who faced trials and tribulations. Justine, mistreated by her mother, was taken in by Victor's aunt and received a good education. Despite her hardships, Justine remains a beloved member of the family.\n",
      "\n",
      "Victor's background is rooted in a close-knit family. His father, a friend of Beaufort, marries Caroline Beaufort after her father's death, and they have Victor, their eldest child, in Naples. Victor's childhood is filled with love and indulgence, shared with his close companion, Elizabeth Lavenza, who was adopted into the family, and his friend Henry Clerval, passionate about chivalry and adventure.\n",
      "\n",
      "Victor's early life is marked by a deep interest in learning the secrets of heaven and earth, influenced by the works of Cornelius Agrippa, Paracelsus, and Albertus Magnus. A thunderstorm and the destruction of an oak tree by lightning shift his interest towards electricity and galvanism. He attends the University of Ingolstadt, where\n"
     ]
    }
   ],
   "source": [
    "prompt_options = {}\n",
    "prompt_options['prompt_type'] = \"summary\"\n",
    "prompt_options['format_type'] = \"narrative\"\n",
    "prompt_options['manual_guidance'] = \"\"\n",
    "prompt_options['style_guide'] = \"\"\n",
    "\n",
    "revised_summary = generate_single_doc_summary(longdoc, prompt_options, AUTO_REFINE=True, DEBUG=True)\n",
    "print (\"Final Summary:\")\n",
    "print (revised_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Data/AOAIFAQ.txt\", 'rb') as file:\n",
    "    # Read the file as a text string\n",
    "    aoaifaq = file.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a new combined summary for 7 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\astalati\\AppData\\Local\\Temp\\ipykernel_23860\\3580541671.py:36: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
      "  worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial summary chunks done!\n",
      "Creating joint summary...\n",
      "Chunk summaries token count: 2305\n",
      "Final merge of summary chunks, merging 7 summaries.\n",
      "First summary:\n",
      "The Azure OpenAI Service, detailed in the document titled \"Azure OpenAI Service – Customer Conversation Guidance & FAQ\" updated on May 23, 2023, encompasses a range of features and announcements, particularly those made at Build 2023. One of the key highlights is the introduction of Provisioned Throughput, which allows customers to reserve and deploy Azure OpenAI model processing capacity for high-volume workloads. This feature is quantified in Provisioned Throughput Units (PTUs), with specific requirements for different models: GPT-3.5 Turbo requires a minimum of 300 PTUs, GPT-4 8k requires 900 PTUs, and GPT-4 32k requires 1800 PTUs. These units can be reserved in increments and committed for either one month or one year. The benefits of this system include cost savings, reserved processing capacity, consistent performance, and flexible commitment periods, making it ideal for consistent, high-volume workloads and production rollouts.\n",
      "\n",
      "To sign up for Provisioned Throughput, customers must contact their Microsoft account team for workload sizing and assessment. The deployment process involves requesting a quota for PTUs, purchasing a commitment tier within 48 hours of approval, and creating provisioned throughput deployments. Scaling deployments is possible by adjusting PTUs, and handling workload spikes can be managed by pairing provisioned throughput with standard, usage-based deployments.\n",
      "\n",
      "Azure OpenAI Service also introduced a quota system starting June 1, 2023, providing granular, self-service control over throughput allocation and eliminating restrictive limits on deployments and resources per region. The quota types include Standard Throughput Units (STUs) for standard deployments and Provisioned Throughput Units (PTUs) for commitment-based deployments. The billing model remains per-1K-token for standard deployments, while provisioned deployments require a commitment plan.\n",
      "\n",
      "The service ensures robust customer data governance, with Microsoft not using customer data to train or improve large language models. Customers own their data, and Microsoft uses it only to provide agreed-upon services. Azure OpenAI Service ensures OpenAI has no access to Microsoft customer data or custom-tuned models, with prompts and completions logged for up to 30 days for content management and debugging. Compliance is maintained through comprehensive enterprise controls, and the service is subject to Microsoft's Data Protection Addendum and service terms.\n",
      "\n",
      "Azure OpenAI Service also supports plugins, which extend its functionality. These plugins are interoperable between OpenAI and Azure OpenAI, with some requiring authentication for full access. Customers\n",
      "Asking the LLM to find weaknesses in this summary...\n",
      "Questions from the LLM:\n",
      "Certainly! Here are the top 10 most important questions about the text that cannot be answered by the text itself:\n",
      "\n",
      "1. What are the specific costs associated with each Provisioned Throughput Unit (PTU) for different models?\n",
      "2. How does the process of contacting the Microsoft account team for workload sizing and assessment work in detail?\n",
      "3. What are the specific steps involved in requesting a quota for PTUs?\n",
      "4. What are the detailed terms and conditions for the one-month and one-year commitment periods for PTUs?\n",
      "5. How does the billing model for provisioned deployments compare to the standard per-1K-token billing model in terms of cost efficiency?\n",
      "6. What are the specific compliance and enterprise controls mentioned for data governance?\n",
      "7. How does the quota system introduced on June 1, 2023, specifically improve throughput allocation and resource management?\n",
      "8. What are the specific plugins supported by Azure OpenAI Service, and what functionalities do they extend?\n",
      "9. What are the authentication requirements for accessing certain plugins?\n",
      "10. How does the service handle data security and privacy for custom-tuned models and customer data beyond the 30-day logging period?\n",
      "\n",
      "These questions delve into the specifics and operational details that the provided text does not cover.\n",
      "Generating answers using 7 chunks.\n",
      "Partial summary chunks done!\n",
      "Creating joint summary...\n",
      "Chunk summaries token count: 2439\n",
      "Final merge of summary chunks, merging 7 summaries.\n",
      "Additional Details:\n",
      "Azure OpenAI Service offers customers access to OpenAI’s models, including GPT-4 (preview), GPT-3, Codex, Embeddings, DALL-E (private preview), and ChatGPT (preview), through the Azure platform. Users can interact with these models via REST APIs, Python SDK, or the web-based interface in the Azure OpenAI Studio. The service reached General Availability on December 14, 2022, with GPT-4 and ChatGPT becoming generally available on May 15, 2023. \n",
      "\n",
      "A key feature of the Azure OpenAI Service is the Provisioned Throughput, which allows customers to reserve and deploy model processing capacity for high-volume workloads. Customers can purchase Provisioned Throughput Units (PTUs) on a monthly or yearly basis, with costs set at $260 per PTU monthly and $2,640 per PTU yearly. The models supported include GPT-35-Turbo, GPT-4 8k, and GPT-4 32k, with minimum PTU requirements and increments varying by model type. For instance, GPT-3.5 Turbo requires a minimum of 300 PTUs, while GPT-4 32k requires a minimum of 1800 PTUs.\n",
      "\n",
      "Provisioned Throughput offers several benefits, including cost savings for consistent, high-volume workloads, reserved processing capacity, consistent performance, flexible commitment periods, and predictable budgeting. It is particularly suitable for customers with consistent, high-volume workloads and latency-sensitive scenarios requiring predictable performance. Customers interested in this feature must contact their Microsoft account team for workload sizing and assessment, request quota from the Usages+Quota tab of Azure AI Studio, and purchase a commitment tier within 48 hours of quota approval.\n",
      "\n",
      "The Azure OpenAI Service is protected by comprehensive enterprise compliance and security controls, and it adheres to Microsoft's Data Protection Addendum and service terms. Customer data is not used to train or improve large language models, and OpenAI does not have access to any Microsoft customer data or custom-tuned models. Prompts and completions are logged and retained for up to 30 days for content management and debugging purposes. The service also includes a built-in content filtering system to filter harmful and inappropriate content, with support for filtering sexual, hate, self-harm, and violent content.\n",
      "\n",
      "Azure OpenAI Service operates under a limited access framework, requiring customers to apply to be onboarded. Once approved, customers can manage their quota and access through the Azure quota framework, which provides transparency\n",
      "Merging details into original summary...\n",
      "Final Summary:\n",
      "The Azure OpenAI Service, detailed in the document titled \"Azure OpenAI Service – Customer Conversation Guidance & FAQ\" updated on May 23, 2023, encompasses a range of features and announcements, particularly those made at Build 2023. One of the key highlights is the introduction of Provisioned Throughput, which allows customers to reserve and deploy Azure OpenAI model processing capacity for high-volume workloads. This feature is quantified in Provisioned Throughput Units (PTUs), with specific requirements for different models: GPT-3.5 Turbo requires a minimum of 300 PTUs, GPT-4 8k requires 900 PTUs, and GPT-4 32k requires 1800 PTUs. These units can be reserved in increments and committed for either one month or one year, with costs set at $260 per PTU monthly and $2,640 per PTU yearly. The benefits of this system include cost savings, reserved processing capacity, consistent performance, and flexible commitment periods, making it ideal for consistent, high-volume workloads and production rollouts.\n",
      "\n",
      "To sign up for Provisioned Throughput, customers must contact their Microsoft account team for workload sizing and assessment. The deployment process involves requesting a quota for PTUs from the Usages+Quota tab of Azure AI Studio, purchasing a commitment tier within 48 hours of approval, and creating provisioned throughput deployments. Scaling deployments is possible by adjusting PTUs, and handling workload spikes can be managed by pairing provisioned throughput with standard, usage-based deployments.\n",
      "\n",
      "Azure OpenAI Service also introduced a quota system starting June 1, 2023, providing granular, self-service control over throughput allocation and eliminating restrictive limits on deployments and resources per region. The quota types include Standard Throughput Units (STUs) for standard deployments and Provisioned Throughput Units (PTUs) for commitment-based deployments. The billing model remains per-1K-token for standard deployments, while provisioned deployments require a commitment plan.\n",
      "\n",
      "The service ensures robust customer data governance, with Microsoft not using customer data to train or improve large language models. Customers own their data, and Microsoft uses it only to provide agreed-upon services. Azure OpenAI Service ensures OpenAI has no access to Microsoft customer data or custom-tuned models, with prompts and completions logged for up to 30 days for content management and debugging. Compliance is maintained through comprehensive enterprise controls, and the service is subject to Microsoft's Data Protection Addendum and service terms. Additionally, the service\n"
     ]
    }
   ],
   "source": [
    "prompt_options = {}\n",
    "prompt_options['prompt_type'] = \"summary\"\n",
    "prompt_options['format_type'] = \"narrative\"\n",
    "prompt_options['manual_guidance'] = \"\"\n",
    "prompt_options['style_guide'] = \"\"\n",
    "\n",
    "revised_summary = generate_single_doc_summary(aoaifaq, prompt_options, AUTO_REFINE=True, DEBUG=True)\n",
    "print (\"Final Summary:\")\n",
    "print (revised_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "prospectusSummaryIndexName = 'summary'\n",
    "selectedTopics = ['Growth Strategy', 'Positive Outlook']\n",
    "summaryPromptTemplate = ''\n",
    "temperature = 0.3\n",
    "tokenLength = 2500\n",
    "fileName = ''\n",
    "topK = 3\n",
    "SearchService = os.getenv(\"SearchService\")\n",
    "SearchKey = os.getenv(\"SearchKey\")\n",
    "indexNs = \"1eac4c6dced74a6cb3657466ecbfc1d6\"\n",
    "fileName = \"actuary-gpt-applications-of-large-language-models-to-insurance-and-actuarial-work.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if summaryPromptTemplate == '':\n",
    "        summaryPromptTemplate = \"\"\"You are an AI assistant tasked with summarizing documents from large documents that contains information about Initial Public Offerings. \n",
    "        IPO document contains sections with information about the company, its business, strategies, risk, management structure, financial, and other information.\n",
    "        Your summary should accurately capture the key information in the document while avoiding the omission of any domain-specific words. \n",
    "        Please generate a concise and comprehensive summary that includes details. \n",
    "        Ensure that the summary is easy to understand and provides an accurate representation. \n",
    "        Begin the summary with a brief introduction, followed by the main points.\n",
    "        Generate the summary with minimum of 7 paragraphs and maximum of 10 paragraphs.\n",
    "        Please remember to use clear language and maintain the integrity of the original information without missing any important details:\n",
    "        {text}\n",
    "\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "                azure_endpoint=os.getenv(\"OpenAiEndPoint\"),\n",
    "                api_version=os.getenv(\"OpenAiVersion\"),\n",
    "                azure_deployment=os.getenv(\"OpenAiChat4o\"),\n",
    "                temperature=0,\n",
    "                api_key=os.getenv(\"OpenAiKey\"),\n",
    "                openai_api_type=\"azure\",\n",
    "                max_tokens=2000)\n",
    "embeddings = AzureOpenAIEmbeddings(azure_endpoint=os.getenv(\"OpenAiEndPoint\"), \n",
    "                                   azure_deployment=os.getenv(\"OpenAiEmbedding\"), api_key=os.getenv(\"OpenAiKey\"), openai_api_type=\"azure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import *\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "import logging\n",
    "from azure.search.documents.models import QueryType\n",
    "from azure.search.documents.indexes.models import (  \n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SimpleField,  \n",
    "    SearchableField,  \n",
    "    SearchIndex,  \n",
    "    SemanticConfiguration,  \n",
    "    SemanticField,  \n",
    "    SearchField,  \n",
    "    SemanticPrioritizedFields,\n",
    "    VectorSearch,  \n",
    "    HnswAlgorithmConfiguration,  \n",
    ")\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt  \n",
    "import openai\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "from azure.search.documents.models import VectorizedQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createProspectusSummary(SearchService, SearchKey, indexName):\n",
    "\n",
    "    indexClient = SearchIndexClient(endpoint=f\"https://{SearchService}.search.windows.net/\",\n",
    "            credential=AzureKeyCredential(SearchKey))\n",
    "    if indexName not in indexClient.list_index_names():\n",
    "        index = SearchIndex(\n",
    "            name=indexName,\n",
    "            fields=[\n",
    "                            SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "                        SearchableField(name=\"fileName\", type=SearchFieldDataType.String, sortable=True,\n",
    "                                        searchable=True, retrievable=True, filterable=True, facetable=True, analyzer_name=\"en.microsoft\"),\n",
    "                        SearchableField(name=\"docType\", type=SearchFieldDataType.String, sortable=True,\n",
    "                                        searchable=True, retrievable=True, filterable=True, facetable=True, analyzer_name=\"en.microsoft\"),\n",
    "                        SearchableField(name=\"topic\", type=SearchFieldDataType.String, sortable=True,\n",
    "                                        searchable=True, retrievable=True, filterable=True, facetable=True, analyzer_name=\"en.microsoft\"),\n",
    "                        SimpleField(name=\"summary\", type=\"Edm.String\", retrievable=True),\n",
    "            ],\n",
    "            semantic_search = SemanticSearch(configurations=[SemanticConfiguration(\n",
    "                name=\"semanticConfig\",\n",
    "                prioritized_fields=SemanticPrioritizedFields(\n",
    "                    title_field=SemanticField(field_name=\"docType\"),\n",
    "                    keywords_fields=[SemanticField(field_name=\"topic\")],\n",
    "                    content_fields=[SemanticField(field_name=\"summary\")]\n",
    "                )\n",
    "            )])\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            print(f\"Creating {indexName} search index\")\n",
    "            indexClient.create_index(index)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        print(f\"Search index {indexName} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSummaryInIndex(SearchService, SearchKey, indexName, fileName, docType, returnFields=[\"id\", \"fileName\", \"docType\", 'topic', \"summary\"]):\n",
    "    searchClient = SearchClient(endpoint=f\"https://{SearchService}.search.windows.net\",\n",
    "        index_name=indexName,\n",
    "        credential=AzureKeyCredential(SearchKey))\n",
    "    \n",
    "    try:\n",
    "        r = searchClient.search(\n",
    "            search_text=\"\",\n",
    "            filter=\"fileName eq '\" + fileName + \"' and docType eq '\" + docType + \"'\",\n",
    "            select=returnFields,\n",
    "            semantic_configuration_name=\"semanticConfig\",\n",
    "            include_total_count=True\n",
    "        )\n",
    "        return r\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTopicSummaryInIndex(SearchService, SearchKey, indexName, fileName, docType, topic, returnFields=[\"id\", \"fileName\", \"docType\", 'topic', \"summary\"]):\n",
    "    searchClient = SearchClient(endpoint=f\"https://{SearchService}.search.windows.net\",\n",
    "        index_name=indexName,\n",
    "        credential=AzureKeyCredential(SearchKey))\n",
    "    \n",
    "    try:\n",
    "        r = searchClient.search(\n",
    "            search_text=\"\",\n",
    "            filter=\"fileName eq '\" + fileName + \"' and docType eq '\" + docType + \"' and topic eq '\" + topic + \"'\",\n",
    "            select=returnFields,\n",
    "            semantic_configuration_name=\"semanticConfig\",\n",
    "            include_total_count=True\n",
    "        )\n",
    "        return r\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt  \n",
    "from openai import OpenAI, AzureOpenAI\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "# Function to generate embeddings for title and content fields, also used for query embeddings\n",
    "def generateEmbeddings(embeddingModelType, text):\n",
    "    if (embeddingModelType == 'azureopenai'):\n",
    "        try:\n",
    "            client = AzureOpenAI(\n",
    "                        api_key = os.getenv('OpenAiKey'),  \n",
    "                        api_version = os.getenv('OpenAiVersion'),\n",
    "                        azure_endpoint = os.getenv('OpenAiEndPoint')\n",
    "                        )\n",
    "\n",
    "            response = client.embeddings.create(\n",
    "                input=text, model=os.getenv('OpenAiEmbedding'))\n",
    "            embeddings = response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            logging.info(e)\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performCogSearch(indexType, embeddingModelType, question, indexName, k, returnFields=[\"id\", \"content\", \"metadata\"] ):\n",
    "    searchClient = SearchClient(endpoint=f\"https://{SearchService}.search.windows.net\",\n",
    "        index_name=indexName,\n",
    "        credential=AzureKeyCredential(SearchKey))\n",
    "    try:\n",
    "        if indexType == \"cogsearchvs\":\n",
    "            r = searchClient.search(  \n",
    "                search_text=question,\n",
    "                vector_queries=[VectorizedQuery(vector=generateEmbeddings(embeddingModelType, question), k_nearest_neighbors=k, fields=\"content_vector\")],  \n",
    "                select=returnFields,\n",
    "                query_type=\"semantic\", \n",
    "                semantic_configuration_name='mySemanticConfig', \n",
    "                query_caption=\"extractive\", \n",
    "                query_answer=\"extractive\",\n",
    "                include_total_count=True,\n",
    "                top=k\n",
    "            )\n",
    "        elif indexType == \"cogsearch\":\n",
    "            #r = searchClient.search(question, filter=None, top=k)\n",
    "            try:\n",
    "                r = searchClient.search(question, \n",
    "                                    filter=None,\n",
    "                                    query_type=QueryType.SEMANTIC, \n",
    "                                    query_speller=\"lexicon\", \n",
    "                                    semantic_configuration_name=\"mySemanticConfig\", \n",
    "                                    top=k, \n",
    "                                    query_caption=\"extractive|highlight-false\")\n",
    "            except Exception as e:\n",
    "                 r = searchClient.search(question, \n",
    "                                filter=None,\n",
    "                                query_type=QueryType.SEMANTIC, \n",
    "                                query_language=\"en-us\", \n",
    "                                query_speller=\"lexicon\", \n",
    "                                semantic_configuration_name=\"default\", \n",
    "                                top=k, \n",
    "                                query_caption=\"extractive|highlight-false\")\n",
    "        return r\n",
    "    except Exception as e:\n",
    "        logging.info(e)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizeTopic(llm, query, embeddingModelType, indexNs, indexType, topK):\n",
    "    if indexType == 'cogsearchvs':\n",
    "        r = performCogSearch(indexType, embeddingModelType, query, indexNs, topK, returnFields=[\"id\", \"content\", \"metadata\"] )          \n",
    "        if r == None:\n",
    "            resultsDoc = [Document(page_content=\"No results found\")]\n",
    "        else :\n",
    "            resultsDoc = [\n",
    "                    Document(page_content=doc['content'], metadata={\"id\": doc['id']})\n",
    "                    for doc in r\n",
    "                    ]\n",
    "        logging.info(f\"Found {len(resultsDoc)} Cog Search results\")\n",
    "\n",
    "    docContent = ' '.join([doc.page_content for doc in resultsDoc])\n",
    "    \n",
    "    if len(docContent) == 0:\n",
    "        return \"I don't know\"\n",
    "    else:\n",
    "        stuffSummary = stuff_it_summary(llm, docContent)\n",
    "        return stuffSummary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeDocs(SearchService, SearchKey, indexName, docs):\n",
    "    logging.info(\"Total docs: \" + str(len(docs)))\n",
    "    searchClient = SearchClient(endpoint=f\"https://{SearchService}.search.windows.net/\",\n",
    "                                    index_name=indexName,\n",
    "                                    credential=AzureKeyCredential(SearchKey))\n",
    "    i = 0\n",
    "    batch = []\n",
    "    for s in docs:\n",
    "        batch.append(s)\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            results = searchClient.merge_or_upload_documents(documents=batch)\n",
    "            succeeded = sum([1 for r in results if r.succeeded])\n",
    "            logging.info(f\"\\tIndexed {len(results)} sections, {succeeded} succeeded\")\n",
    "            batch = []\n",
    "\n",
    "    if len(batch) > 0:\n",
    "        results = searchClient.merge_or_upload_documents(documents=batch)\n",
    "        succeeded = sum([1 for r in results if r.succeeded])\n",
    "        logging.info(f\"\\tIndexed {len(results)} sections, {succeeded} succeeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def processTopicSummary(llm, fileName, indexNs, indexType, prospectusSummaryIndexName, embeddings, embeddingModelType, selectedTopics, \n",
    "                        summaryPromptTemplate, topK, existingSummary):\n",
    "\n",
    "    createProspectusSummary(SearchService, SearchKey, prospectusSummaryIndexName)\n",
    "    topicSummary = []\n",
    "    print(f\"Existing Summary: {existingSummary}\")\n",
    "    if existingSummary == \"true\":\n",
    "        logging.info(f\"Found existing summary\")\n",
    "        r = findSummaryInIndex(SearchService, SearchKey, prospectusSummaryIndexName, fileName, 'prospectus')\n",
    "        for s in r:\n",
    "            topicSummary.append(\n",
    "                {\n",
    "                    'id' : s['id'],\n",
    "                    'fileName': s['fileName'],\n",
    "                    'docType': s['docType'],\n",
    "                    'topic': s['topic'],\n",
    "                    'summary': s['summary']\n",
    "                })\n",
    "    else:\n",
    "        for topic in selectedTopics:\n",
    "            r = findTopicSummaryInIndex(SearchService, SearchKey, prospectusSummaryIndexName, fileName, 'prospectus', topic)\n",
    "            if r.get_count() == 0:\n",
    "                logging.info(f\"Summarize on Topic: {topic}\")\n",
    "                answer = summarizeTopic(llm, topic, embeddingModelType, indexNs, indexType, topK)\n",
    "                if \"I don't know\" not in answer:\n",
    "                    topicSummary.append({\n",
    "                        'id' : str(uuid.uuid4()),\n",
    "                        'fileName': fileName,\n",
    "                        'docType': 'prospectus',\n",
    "                        'topic': topic,\n",
    "                        'summary': answer\n",
    "                })\n",
    "            else:\n",
    "                for s in r:\n",
    "                    topicSummary.append(\n",
    "                        {\n",
    "                            'id' : s['id'],\n",
    "                            'fileName': s['fileName'],\n",
    "                            'docType': s['docType'],\n",
    "                            'topic': s['topic'],\n",
    "                            'summary': s['summary']\n",
    "                        })\n",
    "        mergeDocs(SearchService, SearchKey, prospectusSummaryIndexName, topicSummary)\n",
    "    return topicSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search index summary already exists\n",
      "Existing Summary: False\n"
     ]
    }
   ],
   "source": [
    "summaryTopicData = processTopicSummary(llm, fileName, \"1eac4c6dced74a6cb3657466ecbfc1d6\", \"cogsearchvs\",\n",
    "                                       prospectusSummaryIndexName, embeddings, \"azureopenai\", \n",
    "                            selectedTopics, summaryPromptTemplate, topK, \"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
