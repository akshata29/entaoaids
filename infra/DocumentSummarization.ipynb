{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import display, HTML\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from openai import OpenAI, AzureOpenAI, AsyncAzureOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\astalati\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.azure_openai.AzureChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "                azure_endpoint=os.getenv('OpenAiEndPoint'),\n",
    "                api_version=os.getenv('OpenAiVersion', \"2023-05-15\"),\n",
    "                azure_deployment=os.getenv('OpenAiChat'),\n",
    "                temperature=0,\n",
    "                api_key=os.getenv('OpenAiKey'),\n",
    "                openai_api_type=\"azure\",\n",
    "                max_tokens=2000)\n",
    "embeddings = AzureOpenAIEmbeddings(azure_endpoint=os.getenv('OpenAiEndPoint'), \n",
    "                                   azure_deployment=os.getenv('OpenAiEmbedding'), api_key=os.getenv('OpenAiKey'), openai_api_type=\"azure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.schema.document import Document\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt\n",
    "prompt_template = \"\"\"\\n\\nHuman:  Consider this text:\n",
    "<text>\n",
    "{text}\n",
    "</text>\n",
    "Please create a concise summary in narative format.\n",
    "\n",
    "Assistiant:  Here is the concise summary:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Define LLM chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Define StuffDocumentsChain\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
    "\n",
    "#Note that although langchain often stores douments in small chunks for the \n",
    "#convience of models with smaller context windows, this \"stuff it\" method will\n",
    "#combind all those chunks into a single prompt call.\n",
    "\n",
    "#wrapping in a python function to make it easy to use in other scripts.\n",
    "def stuff_it_summary(doc):\n",
    "    if type(doc) == str:\n",
    "        docs = [Document(page_content=doc)]\n",
    "    return stuff_chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map\n",
    "map_template = \"\"\"\\n\\nHuman: The following is a set of documents\n",
    "<documnets>\n",
    "{docs}\n",
    "</documents>\n",
    "Based on this list of docs, please identify the main themes.\n",
    "\n",
    "Assistant:  Here are the main themes:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"\\n\\nHuman: The following is set of summaries:\n",
    "<summaries>\n",
    "{doc_summaries}\n",
    "</summaries>\n",
    "Please take these and distill them into a final, consolidated summary of the main themes in narative format. \n",
    "\n",
    "Assistant:  Here are the main themes:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "#wrapping in a python function to make it easy to use in other scripts.\n",
    "def map_reduce_summary(doc, DEBUG=False):\n",
    "    if type(doc) == str:\n",
    "        #use the LangChain built in text splitter to split our text\n",
    "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size = 5000,\n",
    "            chunk_overlap  = 200,\n",
    "            length_function = len,\n",
    "            add_start_index = True,\n",
    "        )\n",
    "        split_docs = text_splitter.create_documents([doc])\n",
    "        if DEBUG: print(\"Text was split into %s docs\"%len(split_docs))\n",
    "    return map_reduce_chain.run(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_open_short = './Data/Pickle/hills.pkl'  #2-3 page story, Hills like White Elephants\n",
    "text_to_open_mid = './Data/Pickle/algernon.pkl'  #short story, Flowers for Algernon\n",
    "text_to_open_long = './Data/Pickle/frankenstien.pkl' #short novel, Frankenstine\n",
    "text_to_open_short_factual = './Data/Pickle/elvis.pkl'  #longest wikipedia article, Elvis.\n",
    "\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "with open(text_to_open_short, 'rb') as file:\n",
    "    #note that here, we're loading a single text, but the examples below require each text to be in a list.\n",
    "    doc = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
